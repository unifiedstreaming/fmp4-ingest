
  
                                            R. Mekuria
                                            Unified Streaming
Internet Engineering Task Force             Sam Zheqiang 
Internet-Draft                              Microsoft 
Expires: xxx, 2018                 


Intended status: Best Current Practice      xxx, 2018


          Live Media and Metadata Ingest Protocol
             draft-mekuria-mmingest-00.txt

Abstract

   This Internet draft presents best industry practice for 
   ingesting live media and meta-data content from a 
   live media source. Example receivers include 
   media processing entities, media clouds 
   and content delivery networks. 
   This document defines the binary media format, 
   the preferred transmission methods and protocols 
   and the handling of failovers and redundancy. 
   The ingested content considered includes 
   high quality audio-visual media content, 
   timed meta-data markers, captions, subtitles
   information and overlay graphics. To support advanced 
   live streaming workflows that combine high quality 
   live encoders and advanced media processing entities 
   two profiles of the ingest are presented.
   In the first profile p1 a live media encoder/source 
   ingests media for further downstream media processing. 
   Some examples of such media processing are transcoding, 
   re-packaging, re-encryption and manifest generation. 
   In the second profile p2 the media source also signals 
   information like grouping of the tracks and a streaming 
   manifest and encryption specific information. 
   The second profile is useful for ingesting
   media streaming presentations    
   to entities that do not perform further 
   active media processing that changes the binary 
   media formats. Eamples of such entities are content
   delivery networks or storage nodes that provide 
   delivery and storage functionalities respectively. 
   By separating the two profiles unnecesary overhead 
   is avoided while supporting key use cases of 
   live media ingest.  
   

Status of This Memo

   This Internet-Draft is submitted in full conformance 
   with the provisions of BCP 78 and BCP 79.

   Internet-Drafts are working documents of the Internet 
   Engineering Task Force (IETF).  Note that other groups 
   may also distribute working documents as Internet-Drafts.  
   The list of current Internet-
   Drafts is at http://datatracker.ietf.org/drafts/current/.

   Internet-Drafts are draft documents valid for a maximum
   of six months and may be updated, replaced, or obsoleted 
   by other documents at any time.  It is inappropriate to
   use Internet-Drafts as reference material or to cite 
   them other than as "work in progress."


   
   
   
   
   
   
   
   
   <Mekuria>          Expires xxx 20xx                [Page1]
   


   
Copyright Notice

   Copyright (c) 2018 IETF Trust and the persons identified as the
   document authors.  All rights reserved.
   
   This document is subject to BCP 78 and the IETF Trust's Legal
   Provisions Relating to IETF Documents
   (http://trustee.ietf.org/license-info) in effect on the date of
   publication of this document.  Please review these documents
   carefully, as they describe your rights and restrictions with respect
   to this document.  Code Components extracted from this document must
   include Simplified BSD License text as described in Section 4.e of
   the Trust Legal Provisions and are provided without warranty as
   described in the Simplified BSD License.

Table of Contents

   1.  Introduction
   2.  Conventions and Terminology
   3.  Media Ingest Protocol Behavior
   4.  Formatting Requirements for Timed Text, Captions and Subtitles
   5.  Formatting Requirements for Timed Metadata Markers (profile p1)
   6.  Guidelines for Handling of Media Processing Entity Failover
   7.  Guidelines for Handling of Live Media Source Failover
   8.  Security Considerations
   9.  IANA Considerations
   10. Contributors
   11. References
     11.1.  Normative References
     11.2.  Informative References
     11.3.  URL References
   Author's Address
 
1.  Introduction

   This document describes best practice for media ingest 
   from a live source (e.g. live encoder) towards media processing 
   entities. Examples of media processing entities
   include media packagers, publishing points, streaming origins and 
   content delivery networks. In particular, this document  
   distinguishes active media processing entities and passive media 
   processing entities. Active media processing entities perform 
   media processing such as encryption, packaging, changing (parts of)
   altering the media content and deriving additional information. 
   Passive media processing entities provide pass through functionality 
   and/or delivery and caching functions that do not alter the media 
   content itself. An example of a passive media processing entity
   is a content delivery network (CDN) that provides
   functionalities for efficient delivery of the content. 
   An example of an active media processing entity could 
   be a just-in-time packager or a just-in-time transcoder
   that alter the different media.

  
     <Mekuria>          Expires xxx                [Page2]

     
   To address both of these use cases, two profiles are described 
   in this text. In the first profile p1, the 
   encoder need not have a global overview of what the client
   may receive, while in the second profile p2 this is usually
   the case. Both situations can occur in practical streaming 
   workflows. Practical experience and a deeper look 
   in the protocol exchanges reveals that overhead 
   can be avoided by separating these two use cases 
   in different profiles p1 and p2. 
   
   Diagram 1: Example workflow with media ingest
   Live Media Source -> Media processing entity -> CDN -> End User
   
   Diagram 1 shows the workflow with a live media ingest from a 
   live media source towards an active media processing entity 
   (corresponding to the first profile p1). 
   The media processing entity provides additional 
   processing such as content stitching, encryption, packaging, 
   manifest generation, transcoding etc. In the second
   ingest format the source ingests directly into the Content 
   delivery network or storage nodes. The ingest described in 
   this text defines two profiles p1 and p2 and includes 
   the latest technologies and standards used in the industry. 
   Content considered includes timed metadata, captions, 
   timed text and encoding standards such as HEVC [HEVC]. 
   The protocol and best practices and associated requirements 
   were discussed with stakeholders, including broadcasters, 
   live encoder vendors, content delivery networks, 
   telecommunications companies and cloud service providers.
   We refer to the contributors section 10 for details.
   
   An example of a widely adopted media ingest protocol 
   is the ingest part of Microsoft Smooth Streaming protocol 
   [MS-SSTR]. This protocol connects live encoders to 
   the Microsoft Smooth Streaming server and to 
   the Microsoft Azure cloud. This protocol has shown 
   to be robust, flexible and easy to implement in live 
   encoders. In addition it provided features for 
   high availability and server side redundancy. 

   This document advances over the smooth ingest procotol 
   including lessons learned over the last
   ten years after the initial deployment of smooth streaming 
   in 2009 and several advances on signalling of information
   such as timed metadata markers for content insertion.
   In addition, it incorporates the latest media formats 
   and protocols, making it ready for current and 
   next generation media codecs such as [HEVC] 
   and protocols like MPEG DASH [DASH]. In addition  
   a profile p2 was added for ingest of media 
   streaming presentations.   
   
   In particular, the best practice presented uses 
   HTTP [HTTP] and specifically the HTTP POST method. 
   We still consider the usage of HTTP for media 
   ingest preferable. Smooth streaming [MS-SSTR] 
   and HLS [RFC8216] have shown that HTTP usage 
   can survive the Internet ecosystem. 
   In addition, HTTP based ingest fits well with 
   current  HTTP based streaming protocols 
   and there is good support for HTTP 
   middleboxes, routing etc in the Internet
   Ecosystem. Beyond the usage 
   of HTTP POST method, a key aspect of the 
   ingest spec is the binary media format 
   based on fragmented MPEG-4 
   [ISOBMFF] [CMAF] which allows easy identification 
   of stream boundaries, enabling switching, redundancy, 
   re-transmission making a good fit with the current 
   Internet infrastructures. Many problems in practical 
   deployment often deal with issues related to the binary
   format. In future versions, alternative transmission 
   layers could be considered advancing over HTTP, yet it 
   is expected that the binary layer presented here
   will still provide the same benefits.   
   
   
2.  Conventions and Terminology

   The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
   "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
   document are to be interpreted as described in BCP 14, RFC 2119
   [RFC2119].
   
   This specification uses the following additional terminology.
   ISOBMFF: the ISO Base Media File Format specified in [ISOBMFF].
   ftyp: the filetype and compatibility "ftyp" box as described 
         in the ISOBMFF [ISOBMFF] that describes the "brand" 
   moov: the container box for all metadata "moov" described in the 
         ISOBMFF base media file format [ISOBMFF]
   moof: the movie fragment "moof" box as described in the 
         ISOBMFF  base media file format [ISOBMFF] that describes 
         the metadata of a fragment of media.
   mdat: the media data container "mdat" box contained in 
         an ISOBMFF [ISOBMFF], this box contains the 
         compressed media samples
   kind: the track kind box defined in the ISOBMFF [ISOBMFF] 
         to label a track with its usage
   mfra: the movie fragment random access "mfra" box defined in 
         the ISOBMFF [ISOBMFF] to signal random access samples 
         (these are samples that require no prior 
         or other samples for decoding) [ISOBMFF].
   tfdt: the TrackFragmentDecodeTimeBox box "tfdt" 
         in the base media file format [ISOBMFF] used
         to signal the decode time of the media 
         fragment signalled in the moof box.    
         
<Mekuria>          Expires xxx                [Page3]

    
 
 
   mdhd: The media header box "mdhd" as defined in [ISOBMFF], 
         this box contains information about the media such 
         as timescale, duration, language using ISO 639-2/T codes
         [ISO639-2]      

   pssh: The protection specific system header "pssh" box defined 
         in [CENC] that can be used to signal the content protection 
         information according to the MPEG Common Encryption (CENC)
   sinf: Protection scheme information box "sinf" defined in [ISOBMFF] 
         that provides information on the encryption 
         scheme used in the file
   elng: extended language box "elng" defined in [ISOBMFF] that 
         can override the language information
   nmhd: The null media header Box "nmhd" as defined in [ISOBMFF] 
         to signal a track for which no specific 
         media header is defined, often used for metadata tracks
   HTTP: Hyper Text Transfer Protocol, 
                version 1.1 as specified by [RFC2626]
   HTTP POST: Command used in the Hyper Text Transfer Protocol for 
              sending data from a source to a destination [RFC2626]
   fragmentedMP4stream: stream of [ISOBMFF] fragments 
           (moof and mdat), a more precise definition will follow
   POST_URL: Target URL of a POST command in the HTTP protocol 
             for posting data from a source to a destination.
   TCP: Transmission Control Protocol (TCP) as defined in [RFC793]
   URI_SAFE_IDENTIFIER: identifier/string 
          formatted according to [RFC3986]
   Connection: connection setup between a host and a source.
   Live stream event: the total media broadcast stream of the ingest.
   (Live) encoder: entity performing live encoding and producing 
   a high quality encoded stream, can serve as Media ingest source
   (Media) Ingest source: a media source ingesting media content 
   , typically a live encoder but not restricted to this, 
   the media ingest source could by any type of media ingest 
   source such as a stored file that is send in partial chunks
   Publishing point: entity used to publish the media content, 
   consumes/receives the incoming media ingest stream
   Media processing entity: entity used to process media content, 
   receives/consumes a media ingest stream.
   Media processing function: Media processing entity   

3.  Media Ingest Protocol Behavior

   The specification uses multiple HTTP POST and/or PUT requests 
   to transmit an optional manifest followed by encoded media data 
   packaged in fragmented [ISOBMFF]. The subsequent posted segments 
   correspond to those decribed in the manifest. Each HTTP POST sends 
   a complete manifest or media segment towards the processing entity. 
   The sequence of POST commands starts with the manifest and init 
   segments that includes header boxes (ftyp and moov boxes). 
   It continues with the sequence of segments 
   (combinations of moof and mdat boxes). The two profiles 
   are defined as p1 for ingest to active media processing entities
   and p2 for ingest to passive entities. 
   A benefit of profile p1 is that is will allow downstream
   processing of the media enabling advanced streaming platforms.
   A benefit of profile p2 is that is will enable simultaneous
   ingest of MPEG DASH and HLS presentations using common media 
   segments such as based on fragmented MPEG-4 format [CMAF]   
  
<Mekuria>          Expires xxx                [Page4]

   


   An example of a POST URL 
   targeting the publishing point is:
   http://HostName/presentationPath/manifestpath
   /rsegmentpath/Identifier

   The PostURL the syntax is defined as follows using the 
   IETF RFC 5234 ANB [RFC5234] to specify the structure.

   PostURL = Protocol ://BroadcastURL Identifier
   Protocol = "http" / "https"
   BroadcastURL = HostName "/" PresentationPath
   HostName = URI_SAFE_IDENTIFIER
   PresentationPath = URI_SAFE_IDENTIFIER
   ManifestPath = URI_SAFE_IDENTIFIER
   Rsegmentpath = URI_SAFE_IDENTIFIER
   Identifier = segment_file_name
   
   In this PostURL the HostName is typically the hostname of the 
   media processing entity or publishing point. The presentation path 
   is the path to the specific presentation at the publishing point. 
   The manifest path can be used to signal the specific manifest of 
   the presentation. The rsegmentpath can be a different optional 
   extended path based on the relative paths in the manifest file. 
   The identifier describes the filename of the segment as described 
   in the manifest. The live source sender first sends the manifest 
   to the path http://hostname/presentationpath/ allowing 
   the receiving entity to setup reception paths for the following 
   segments and manifests for profile p2. 
   In case no manifest is used any POST_URL
   setup for media ingest such as http://hostname/presentationpath/
   can be used as recommended for profile  . 
   The fragmentedMP4stream can be defined
   using the IETF RFC 5234 ANB [RFC5234] as follows.
   
   fragmentedMP4stream = headerboxes fragments
   headerboxes = ftyp moov
   fragments = X fragment
   fragment = Moof Mdat
    
   The communication between the live encoder/media ingest source 
   and the receiving media procesing entity follows the following 
   requirements for the two profiles. The requirements apply to 
   both profiles p1 and p2 unless it is specifically indicated
   to which profile the statement applies. In future 
   versions of the draft we will present the profiles in a 
   more distinct way such as in seperate sections.
   yet as there is significant overlap this interleaved
   way of presenting the profiles was chosen for now.
   
   1. The live encoder or ingest source communicates to 
      the publishing point/processing entity using the HTTP 
      POST method as defined in the HTTP protocol [RFC2626],
      or in the case for manifest updates the HTTP PUT Method. 
   2. The live encoder or ingest source SHOULD start
      by sending an HTTP POST request with an empty "body"
      (zero content length) by using the POSTURL 
	  when using profile p1. 
      This can help the live encoder or media 
      ingest source to quickly detect whether the 
      live ingest publishing point is valid, 
      and if there are any authentication 
	  or other conditions required.	  
      
<Mekuria>          Expires xxx                [Page5]

     

      
   3. The live encoder/media source SHOULD use secured 
       transmission using HTTPS protocol 
      as specified in [RFC2818] for connecting  
       to the receiving media processing entity 
      or publishing point. 
   4. In case HTTPS protocol is used, 
      basic authentication HTTP AUTH [RFC7617] 
      or better methods like 
      TLS client certificates SHOULD be used to 
      secure the connection.  
   5. As compatibility profile for the TLS encryption 
      we recommend the mozzilla 
      intermediate compatibility profile which is supported 
      in many available implementations [MozillaTLS]. 
   6. Before sending the segments 
      based on fragmentedMP4Stream the live encoder/source
      MUST send a manifest when using profile p2 
      with the following the limitations/constraints.
   6a. Only relative URL paths to be used for each segment
   6b. Only unique paths are used for each new presentation
   6c. In case the manifest contains these relative paths, 
      these paths SHOULD be used in combination with the 
      POST_URL + relative URLs 
      to POST each of the different segments from 
      the live encoder or ingest source 
      to the processing entity. 

  7. The live encoder MAY send an updated versions of the manifest,
     when using profile p2, this manifest cannot override current 
	 settings and relative paths or break currently running and 
	 incoming POST requests. The updated manifest can only be 
	 slightly different from the one that was send previously, 
	 e.g. introduce new segments available or event messages. 
	 The updated manifest SHOULD be send using a PUT request 
	 instead of a POST request.
         
     Note: this manifest will be useful for passive media processing 
           entities mostly, for ingest towards active media processing 
           entities this manifest could be avoided and information 
           is signalled through the boxes available in the ISOBMFF.
    
  8. The encoder or ingest source MUST handle any error or failed 
     authentication responses received from the media processing 
     entity such as 403 (forbidden), 400 bad request, 415 
     unsupported media type, 412 not fulfilling conditions
         
<Mekuria>          Expires xxx                [Page6]

 

  9. In case of a 412 not fullfilling conditions or 415 
      unsupported media type, the live source/encoder 
	  MUST resend the init segment 
      consisting of a "moov" and "ftyp" box.
  10. The live encoder or ingest source SHOULD start 
      a new HTTP POST segment request sequence with the 
      init segment including header boxes "ftyp" and "moov"
	  when using profile p1
  11. Following media segment requests SHOULD be corresponding 
      to the segments listed in the manifest if a manifest was sent
	  when using profile p2. The requests when using profile p2
      SHOULD use individual POST requests per segment.	  
  12. The payload of each request MAY start with the header boxes 
      "ftyp" and "moov", followed by segments which consist of 
      a combination of "moof" and "mdat" boxes when using profile
	  p1. 
      
      Note that the "ftyp", and "moov" boxes (in this order) MAY be 
      transmitted with each request, especially if the encoder must 
      reconnect because the previous POST request was terminated 
      prior to the end of the stream with a 412 or 
      415 message. Resending the "moov" and "ftyp" boxes 
      allows the receiving entitity to recover the init segment 
      and the track information needed for interpretting the content.
	  
  13. The encoder or ingest source SHOULD use chunked transfer 
      encoding option of the HTTP POST command [RFC2626]
      as it might be difficult to predict the entire content length 
      of the segment. This can also be used for example to support use 
      cases that require low latency.
  14. The encoder or ingest source SHOULD use individual HTTP POST 
      commands [RFC2626] for uploading media segments when ready
	  when using profile p2. 
  15. If the HTTP POST request terminates or times out with a TCP 
      error prior to the end of the stream, the encoder MUST issue 
      a new POST request by using a new connection, and follow the 
      preceding requirements. Additionally, the encoder MAY resend 
      the previous segment that was already sent again.
  16. In case fixed length POST Commands are used, the live source 
      entity MUST resend the segment to be posted decribed 
	  in the manifest entirely in case of responses HTTP 400,
	  412 or 415 together with the init segment consisting 
	  of "moov" and "ftyp" boxes.
  17. In case the live stream event is over the live media 
      source/encoder should signal 
      the stop by transmitting an empty "mfra" box 
      towards the publishing point/processing entity 
	  when using profile p1.
  18. The trackFragmentDecodeTime box "tfdt" box
      MUST be present for each segment posted.
  19. The ISOBMFF media fragment duration SHOULD be constant,
      when using profile p1, this is   
      to reduce the size of the client manifests. 
      A constant MPEG-4 fragment duration also improves client 
      download heuristics through the use of repeat tags. 
      The duration MAY fluctuate to compensate 
      for non-integer frame rates. By choosing an appropriate 
      timescale (a multiple of the frame rate is recommended) 
      this issue can be avoided. For profile p2 segment 
	  durations MAY vary.
      
<Mekuria>          Expires xxx                [Page6]

 


  20. The MPEG-4 fragment durations SHOULD be between 
      approximately 1 and 6 seconds.
  21. The fragment decode timestamps "tfdt" of fragments in the 
      fragmentedMP4stream and the indexes base_media_decode_ time 
      SHOULD arrive in increasing order for each of the different 
      tracks/streams that are ingested for profile p1.
  22. The segments formatted as fragmented MP4 stream SHOULD use 
      a timescale for video streams based on the framerate 
      and 44.1 KHz or 48 KHz for audio streams 
      or any another timescale that enables integer 
      increments of the decode times of 
      fragments signalled in the "tfdt" box based on this scale.
  23. The manifest SHOULD be used to signal the language of the stream
      for profile p2, which SHOULD also be signalled in the 
	  "mdhd" box or "elng" boxes in the init segment and/or moof headers 
	  ("mdhd") for profile p1.
  24. The manifest SHOULD be used to signal encryption specific 
      information for profile p2, which SHOULD be signalled 
	  in the "pssh","schm" and "sinf" boxes 
	  in the segments of 
      the init segment and media segments, 
	  when using profile p1
  25. The manifest SHOULD be used to signal information 
      about the different 
      tracks such as the durations,  media encoding types, 
      content types for profile p2, which SHOULD instead be 
	  signalled in the "moov" box in the init segment 
	  or the "moof" box in the media segments for profile p1
  26. The manifest SHOULD be used to signal information 
      about the timed text, images and sub-titles in adaptation 
      sets for profile p2 and this information 
	  SHOULD be signalled in the "moov" box in the init segment
	  for profile p1,for more information see the next section.
  27. Segments posted towards the media procesing entity SHOULD 
      contain the bitrate "btrt" box specifying the target 
	  bitrate of the segments and the "tfdt" box specifying 
	  the fragments decode time and the "tfhd" box 
	  specifying the track id when using profile p1.
  28. The live encoder/media source SHOULD repeatedly resolve 
      the Hostname to adapt to changes in the IP to Hostname mapping
      such as for example by using the dynamic naming system 
      DNS [RFC1035] or any other system that is in place.
  29. The Live encoder media source MUST update the IP to hostname 
      resolution respecting the TTL (time to live) from DNS 
      query responses, this will enable better resillience 
      to changes of the IP address in large scale deployments 
      where the IP adress of the publishing point media 
      processing nodes may change frequenty.
  30. To support the ingest of live events with low latency, 
      shorter segment and fragment durations MAY be used 
      such as segments with a duration of 1 or 2 seconds 
	  and chunked transfer coding.
  31. The live encoder/media source SHOULD use a separate TCP
      connection for ingest of each different bit-rate 
      tracks ingested 

      
      
<Mekuria>          Expires xxx            [Page8]

  

      
4. Formatting Requirements for Timed Text, Captions and Subtitles

The specification supports ingest of timed text, 
images, captions and subtitles. We follow the normative 
reference [MPEG-4-30] in this section. 

  1. The tracks containing timed text, images, captions 
  or subtitles SHOULD be signalled in the manifest by 
  an adaptationset with the different segments 
  containing the data of the track when using profile p2.  
  2. The segment data SHOULD be posted to the URL 
  corresponding to the path in the manifest for the segment 
  for profile p2, else they MUST be posted 
  towards the original POST_URL for profile p1  
  3. The track will be a sparse track signalled by a null media header 
  "nmhd" containing the timed text, images, captions corresponding 
  to the recommendation of storing tracks in fragmented MPEG-4 [CMAF]
  4. Based on this recommendation the trackhandler "hdlr" shall 
  be set to "text" for WebVTT and "subt" for TTML following [MPEG-4-30] 
  5. In case TTML is used the track must use the XMLSampleEntry 
  to signal sample description of the sub-title stream [MPEG-4-30] 
  6. In case WebVTT is used the track must use the WVTTSampleEntry 
  to signal sample description of the text stream
  7. These boxes SHOULD signal the mime type and specifics as 
  described in [CMAF] sections 11.3 ,11.4 and 11.5
  8. The boxes described in 3-7 must be present in the init 
  segment ("ftyp" + "moov") for the given track
  9. subtitles in CTA-608 and CTA-708 can be transmitted 
  following the recommendation section 11.5 in [CMAF] via 
  SEI messages in the video track when using profile p1
  10. The "ftyp" box in the init segment for the track 
      containing timed text, images, captions and sub-titles 
      MAY use signalling using CMAF profiles based on [CMAF] 
   
   10a. WebVTT   Specified in 11.2 ISO/IEC 14496-30 
        [MPEG-4-30] 'cwvt'
   10b.TTML IMSC1 Text  Specified in 11.3.3 [MPEG-4-30] 
       IMSC1 Text Profile   'im1t'
   10c.TTML IMSC1 Image Specified in 11.3.4 [MPEG-4-30] 
       IMSC1 Image Profile  'im1i'
   10d. CEA  CTA-608 and CTA-708 Specified in 11.4 [MPEG-4-30] 
       Caption data is embedded in SEI messages in video track; 
      'ccea'
  11. The segments of the tracks containing Timed Text, Images,
      Captions and Sub-titles SHOULD use the bit-rate box "btrt" to 
      signal bit-rate of the track in each segment when using 
	  profile p1. 
 






 
 <Mekuria>          Expires xxx                [Page9]

  


5. Formatting Requirements for Timed Metadata (profile p1)

  This section discusses the specific formatting requirements 
  for ingest of timed metadata related to events and markers for 
  ad insertion or other timed metadata relating to the media
  content such as information about the content. This section
  only aplies to p1 where the receiver uses this information 
  for active media processing such as server side ad insertion.
  When delivering a live streaming presentation with a 
  rich client experience, often it is necessary 
  to transmit time-synced events, metadata or 
  other signals in-band with the main 
  media data. This type of data has an arrival time that is 
  signalled by the transmitter. This type of data also 
  of often have a presentation time, which is the 
  time when the message is applied in the media 
  presentation. An example of these are opportunities 
  for dynamic live ad insertion signalled by SCTE-35 markers. 
  This type of  event signalling is different 
  from regular audio/video information 
  because of its sparse nature. In this case, 
  the signalling data usually does not 
  happen continuously, and the intervals can 
  be hard to predict. Examples of timed metadata are ID3 tags 
  [ID3v2], SCTE-35 markers [SCTE-35] and DASH emsg 
  messages defined in section 5.10.3.3 of [DASH]. For example, 
  DASH Event messages contain a schemeIdUri that defines 
  the payload of the message. Table 1 provides some 
  example schemes in DASH event messages and Table 2 
  illustrates an example of a SCTE-35 marker stored 
  in a DASH emsg. The presented approach allows ingest of 
  timed metadata from different sources, 
  possibly on different locations by embedding them in 
  sparse metadata tracks.

Table 1 Example of DASH emsg schemes  URI

Scheme URI               | Reference
-------------------------|------------------
urn:mpeg:dash:event:2012 | [DASH], 5.10.4
urn:dvb:iptv:cpm:2014    | [DVB-DASH], 9.1.2.1 
urn:scte:scte35:2013:bin | [SCTE-35] 14-3 (2015), 7.3.2
www.nielsen.com:id3:v1   | Nielsen ID3 in MPEG-DASH

Table 2 example of a SCTE-35 marker embedded in a DASH emsg
Tag                     |          Value
------------------------|-----------------------------
scheme_uri_id           | "urn:scte:scte35:2013:bin"
Value                   | the value of the SCTE 35 PID
Timescale               | positive number
presentation_time_delta | non-negative number expressing splice time
                        | relative  to tfdt 
event_duration          | duration of event
                        | "0xFFFFFFFF" indicates unknown duration
Id                      | unique identifier for message
message_data            | splice info section including CRC






<Mekuria>          Expires xxx                [Page10]

  


  The following steps are recommended for timed metadata 
  ingest related to events, tags, ad markers and 
  program information:
  
  1. Create a fragmentedMP4stream that conveys the metadata
     , the media handler type is "meta", 
     the track handler box is a null media header box "nmhd".
  2. The metadata stream applies to the media streams 
     in the presentation ingested to the POST_URL corresponding
     to the ingest/publishing point 
  3. The URIMetaSampleEntry entry contains, in a URIbox, 
     the URI following the URI syntax in [RFC3986] 
     defining the form  of the metadata 
     (see the ISO Base media file format 
     specification [ISOBMFF]). For example, the URIBox 
     could contain for ID3 tags  [ID3v2] 
     the URL  http://www.id3.org or
     or urn:scte:scte35:2013a:bin 
     for scte 35 markers [SCTE35]
  4. The timescale of the metadata should match the value 
     specified in the media header box "mdhd" 
     for timed metadata.
  5. The Arrival time is signalled in the "tfdt" box 
     of the track fragment  as the basemediadecode
     time, this time is often different
     from the media presentation time, which is occurs
     when a message is applied. The duration of 
     a metadata fragment can be set to zero, 
     letting it be determined by the 
     time a next metadata fragment is received.
  6. The kind box can be used to group metadata and 
     other tracks posted to the POST_URL based on their
     properties, this is corresponding
     to switching sets in [CMAF] that can be detected
     in a similar manner.
  7. All Timed Metadata samples SHOULD be sync samples [ISOBMFF], 
    defining the entire set of metadata for the time interval 
    they cover. Hence, the sync sample table box SHOULD 
    not be present.
  

      
        
 <Mekuria>          Expires xxx                 [Page11]

  

      
  8. The metadata segment becomes available to the 
     publishing  point/processing entity when the corresponding 
     track fragment that has an equal or larger timestamp 
     compared to teh arival time signalled in the tfdt. 
     For example, if the sparse fragment 
     has a timestamp of t=1000, it is expected that after the 
     publishing point/processing entity sees "video" 
    (assuming the parent track name is "video") 
    fragment timestamp 1000 or beyond, it can retrieve the 
    sparse fragment from the binary payload.
  9.  The payload of sparse track fragments is conveyed 
      in the mdat box as sample information. This enables 
      muxing of the metadata tracks. XML based metadata 
      can for example be coded as base64 as in [SCTE35]
  10. All Timed Metadata samples SHOULD be sync samples [ISOBMFF],
    defining the entire set of metadata for the time interval
    they cover. Hence, the sync sample table box is not present.

6. Guidelines for Handling of Media Processing Entity Failover

  Given the nature of live streaming, good failover support is 
  critical for ensuring the availability of the service. 
  Typically, media services are designed to handle various types 
  of failures, including network errors, server errors, and storage 
  issues. When used in conjunction with proper failover 
  logic from the live encoder side, customers can achieve 
  a highly reliable live streaming service from the cloud. 
  In this section, we discuss service failover scenarios. 
  In this case, the failure happens somewhere within the service, 
  and it manifests itself as a network error. Here are some 
  recommendations for the encoder implementation for handling 
  service failover:
  1.    Use a 10-second timeout for establishing the
     TCP connection. 
    If an attempt to establish the connection takes longer 
    than 10 seconds, abort the operation and try again.
  2.    Use a short timeout for sending the HTTP requests. 
    If the target segment duration is N seconds, use a send 
    timeout between N and 2 N seconds; for example, if 
    the segment duration is 6 seconds, 
    use a timeout of 6 to 12 seconds. 
    If a timeout occurs, reset the connection, 
    open a new connection, 
    and resume stream ingest on the new connection. 
    This is needed to avoid latency introduced
    by failing connectivity in the workflow.
  3. completely resend segments from the ingest source 
    for which a connection was terminated early
  4.    We recommend that the encoder or ingest source 
    does NOT limit the number of retries to establish a
    connection or resume streaming after a TCP error occurs.
    

<Mekuria>          Expires xxx                 [Page12]



  5.    After a TCP error:
   a. The current connection MUST be closed, 
      and a new connection MUST be created 
      for a new HTTP POST request.
   b. The new HTTP POST URL MUST be the same 
      as the initial POST URL for the 
      segment to be ingested.
   c. The new HTTP POST MUST include stream 
      headers ("ftyp", and "moov" boxes) when 
	  using p1 that are 
      identical to the stream headers in the 
      initial POST request for fragmented media ingest.

  6.  The encoder or ingest source SHOULD terminate 
    the HTTP POST request if data is not being sent 
    at a rate commensurate with the MP4 segment duration. 
    An HTTP POST request that does not send data can 
    prevent publishing points or media processing entities 
    from quickly disconnecting from the live encoder or 
    media ingest source in the event of a service update. 
    For this reason, the HTTP POST for sparse  
    tracks SHOULD be short-lived, terminating as soon as 
    the sparse fragment is sent. 
   In addition this draft defines responses to the 
   POST requests in order to signal the live media source its status.  
   7.  In case the media processing entity cannot process the manifest 
    or segment POST request due to authentication or permission 
    problems then it can return a permission denied HTTP 403 
   8.  In case the media processing entity can process the manifest 
    or segment it returns HTTP 200 OK or 202 Accepted
   9.  In case the media processing entity can process 
    the manifest or segment in the POST request body but finds 
    the media type cannot be supported it returns HTTP 415 
    unsupported media type
   10. In case an unknown error happened during
       the processing of the HTTP 
        POST request a HTTP 400 Bad request is returned 
   11. In case the media processing entity cannot 
       proces a segment posted 
       due to missing init segment, a HTTP 412 
       unfulfilled condition can be returned
   12. In case a media source receives an HTTP 412 response, 
       it SHOULD resend the manifest for profile p2 
	   and "ftyp" and "moov" boxes for the track when using
	   profile p1.    





       
<Mekuria>          Expires xxx                 [Page13]




An example of media ingest with failure and HTTP 
responses is shown in the following figure:


||===============================================================|| 
||=====================            ============================  || 
||| live media source |            |  Media processing entity |  ||
||=====================            ============================  ||
||        ||                                     ||              ||
||===============Initial Manifest Sending========================||
||        ||                                     ||              ||
||        ||-- POST /prefix/media.mpd  -------->>||              ||
||        ||          Succes                     ||              ||
||        || <<------ 200 OK --------------------||              ||
||        ||      Permission denied              ||              ||
||        || <<------ 403 Forbidden -------------||              ||
||        ||             Bad Request             ||              ||
||        || <<------ 400 Forbidden -------------||              ||
||        ||         Unsupported Media Type      ||              ||
||        || <<------ 415 Unsupported Media -----||              ||
||        ||                                     ||              ||
||==================== Segment Sending ==========================|| 
||        ||-- POST /prefix/chunk.cmaf  ------->>||              ||
||        ||          Succes/Accepted            ||              ||
||        || <<------ 200 OK --------------------||              ||
||        ||          Succes/Accepted            ||              ||
||        || <<------ 202 OK --------------------||              ||
||        ||      Premission Denied              ||              ||
||        || <<------ 403 Forbidden -------------||              ||
||        ||             Bad Request             ||              ||
||        || <<------ 400 Forbidden -------------||              ||
||        ||         Unsupported Media Type      ||              ||
||        || <<------ 415 Forbidden -------------||              ||
||        ||         Unsupported Media Type      ||              ||
||        || <<-- 412 Unfulfilled Condition -----||              ||
||        ||                                     ||              ||
||        ||                                     ||              ||
||=====================            ============================  || 
||| live media source |            |  Media processing entity |  ||
||=====================            ============================  ||
||        ||                                     ||              ||
||===============================================================|| 











<Mekuria>          Expires xxx                 [Page13]




7. Guidelines for Handling of Live Media Source Failover
 
  Encoder or media ingest source failover is the second type
  of failover scenario that needs to be addressed for end-to-end 
  live streaming delivery. In this scenario, the error condition 
  occurs on the encoder side. The following expectations apply 
  to the live ingestion endpoint when encoder failover happens:
  
  1.    A new encoder or media ingest source instance 
        SHOULD be created to continue streaming
  2.    The new encoder or media ingest source MUST use 
        the same URL for HTTP POST requests as the failed instance.
  3.    The new encoder or media ingest source POST request 
        MUST include the same header boxes moov 
        and ftyp as the failed instance and 
		the same manifest for profile p2.
  4.    The new encoder or media ingest source 
        MUST be properly synced with all other running encoders 
        for the same live presentation to generate synced audio/video 
        samples with aligned fragment boundaries. 
        This implies that UTC timestamps 
        for fragments in the "tdft" match between decoders, 
        and encoders start running at
        an appropriate segment boundary.
  5.    The new stream MUST be semantically equivalent 
        with the previous stream, and interchangeable 
        at the header and media fragment levels.
  6.    The new encoder or media ingest source SHOULD 
        try to minimize data loss. The basemediadecodetime tdft 
        of media fragments SHOULD increase from the point where 
        the encoder last stopped. The basemediadecodetime in the 
        "tdft" box SHOULD increase in a continuous manner, but it 
        is permissible to introduce a discontinuity, if necessary. 
        Media processing entities or publishing points can ignore 
        fragments that it has already received and processed, so 
        it is better to error on the side of resending fragments 
        than to introduce discontinuities in the media timeline.
		This recommendation applies to profile p1.

8.  Security Considerations

   No security considerations except the ones mentioned 
   in the preceding text. Further
   security considerations will be updated 
   when they become known.

9.  IANA Considerations
   
  This memo includes no request to IANA.

10.  Contributors


Will Law Akamai 
James Gruessing BBC R&D
Kevin Moore Amazon AWS Elemental
Kevin Johns CenturyLink 
John Deutscher Microsoft 
Patrick Gendron Harmonic Inc. 
Nikos Kyriopoulos MediaExcel
Rufael Mekuria Unified Streaming
Sam Zheqiang Microsoft
Arjen Wagenaar Unified Streaming 
Dirk Griffioen Unified Streaming 
Matt Poole ITV 
Alex Giladi Comcast
                
<Mekuria>          Expires xxx                 [Page14]




11.  References
   

11.1.  Normative References

    [RFC2119] Bradner, S., "Key words for use in RFCs to Indicate
              Requirement Levels", BCP 14, RFC 2119, March 1997.

    [DASH]  MPEG ISO/IEC JTC1/SC29 WG11, "ISO/IEC 23009-1:2014: 
            Dynamic adaptive streaming over HTTP (DASH) -- Part 1: 
            Media presentation description and segment formats," 2014.
    
    [SCTE-35] Society of Cable Television Engineers, 
              "SCTE-35 (ANSI/SCTE 35 2013) 
               Digital Program Insertion Cueing Message for Cable," 
               SCTE-35 (ANSI/SCTE 35 2013).
               
    [ISOBMFF] MPEG ISO/IEC JTC1/SC29 WG11, " Information technology  
              -- Coding of audio-visual objects Part 12: ISO base 
              media file format ISO/IEC 14496-12:2012" 
  
    [HEVC]    MPEG ISO/IEC JTC1/SC29 WG11, 
              "Information technology -- High efficiency coding 
              and media delivery in heterogeneous environments 
              -- Part 2: High efficiency video coding", 
              ISO/IEC 23008-2:2015, 2015.
        
    [RFC793]  J Postel IETF DARPA, "TRANSMISSION CONTROL PROTOCOL,"
               IETF RFC 793, 1981. 
               
    [RFC3986] R. Fielding, L. Masinter, T. Berners Lee, 
              "Uniform Resource Identifiers (URI): Generic Syntax," 
               IETF RFC 3986, 2004.
               
    [RFC1035] P. Mockapetris, 
              "DOMAIN NAMES - IMPLEMENTATION AND SPECIFICATION" 
              IETF RFC 1035, 1987.
          
    [CMAF]   MPEG ISO/IEC JTC1/SC29 WG11, "Information technology 
             (MPEG-A) -- Part 19: Common media application 
             format (CMAF) for segmented media," 
             MPEG, ISO/IEC International standard
           
    [RFC5234] D. Crocker "Augmented BNF for Syntax Specifications: 
              ABNF"  IETF RFC 5234 2008    
    
    [CENC]   MPEG ISO/IEC JTC1 SC29 WG11 "Information technology -- 
             MPEG systems technologies -- Part 7: Common encryption 
             in ISO base media file format files"
             ISO/IEC 23001-7:2016
    
	[RFC8216] R. Pantos, W. May "HTTP Live Streaming", August 2018 (last acessed)


 <Mekuria>          Expires xxx                [Page15]


    
    
    [MPEG-4-30] MPEG ISO/IEC JTC1 SC29 WG11 
              "ISO/IEC 14496-30:2014 Information technology 
              Coding of audio-visual objects -- Part 30": 
              Timed text and other visual overlays in 
              ISO base media file format
              
   [ISO639-2] ISO 639-2  "Codes for the Representation of Names
              of Languages -- Part 2 ISO 639-2:1998"
              
   [DVB-DASH] ETSI Digital Video Broadcasting 
               "MPEG-DASH Profile for Transport of ISOBMFF
               Based DVB Services over IP Based Networks" 
               ETSI TS 103 285
    
   [RFC7617] J Reschke "The 'Basic' HTTP Authentication Scheme"
             IETF RFC 7617 September 2015
             
11.2.  Informative References

    [RFC2626]  R. Fielding et al 
             "Hypertext Transfer Protocol HTTP/1.1", 
             RFC 2626 June 1999
    
    [RFC2818] E. Rescorla RFC 2818 HTTP over TLS 
             IETF RFC 2818 May 2000

                            
11.3.  URL References

   [fmp4git]    Unified Streaming github fmp4 ingest, 
                "https://github.com/unifiedstreaming/fmp4-ingest".
   
   [MozillaTLS] Mozilla Wikie Security/Server Side TLS 
                https://wiki.mozilla.org/Security/Server_Side_TLS
                #Intermediate_compatibility_.28default.29 
                (last acessed 30th of March 2018)
                
    [ID3v2]      M. Nilsson  "ID3 Tag version 2.4.0 Main structure" 
                http://id3.org/id3v2.4.0-structure
                November 2000 (last acessed 2nd of May 2018)
				
	[MS-SSTR]   Smooth streaming protocol
              	https://msdn.microsoft.com/en-us/library/ff469518.aspx
                last updated March 16 2018 (last acessed June 11 2018)
Author's Address

   Rufael Mekuria (editor)
   Unified Streaming
   Overtoom 60 1054HK 

   Phone: +31 (0)202338801
   E-Mail: rufael@unified-streaming.com
   
   Sam Geqiang Zhang 
   Microsoft
   E-mail: Geqiang.Zhang@microsoft.com

   
     
<Mekuria>          Expires xxx                 [Page16]

   
